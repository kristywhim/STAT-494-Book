{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-13T22:01:00-06:00"
    },
    {
      "path": "ancestry.html",
      "title": "Ancestry in GWAS",
      "description": "How would our ancestry influence GWAS analysis? Why it is problematic?",
      "author": [],
      "contents": "\r\nAncestry\r\nIntro\r\nAnother cool aspect of genetics is ancestry. As generation passes, each\r\nindividual may carry similar but still different genetic ancestral\r\ncomposition as their families (like hybrids). Some genetic companies\r\n(i.e. 23andMe) sell kits for customers to learn more about their\r\ngenetics, both health wise and ancestry wise.\r\nSo how does this area apply to statistical genetics? Well, ancestry has\r\nits connection with GWAS. Besides, scientists can apply mathematical\r\nmethods to better infer one’s ancestry by machine learning, in other\r\nwords, we can guess estimation of one’s origin!\r\nAncestry can be a confounder in GWAS\r\nGWAS analyzes the statistical association between SNPs and our trait of\r\ninterest. In observational studies, ancestry can actually be a\r\nconfounder of GWAS. Confounder is a variable that both associates with\r\nthe predictor and the response variable.\r\nAbove is an simple DAG graphPeople with different ancestry can have very different performance on\r\ngenotype: for example, for a SNP, people with European ancestry may have different allele\r\nfrequencies and different distribution of alleles compared to people with Asian ancestry.\r\nAncestry may also associate with the trait. Therefore, if we do not\r\nstratify and control for the confounding variable, we may end up having\r\na spurious outcome in GWAS. This concept is especially important to keep\r\nin mind when carrying out observational studies.\r\nAfter adjusting for ancestry, our GWAS model (marginal regression) would\r\nlook like:\r\n\\[\r\nE[y|x_j,A] = \\beta_0 +\\beta_j x_j+\\gamma A\r\n\\]\r\nwhere:\r\ny is the trait\r\n\\(x_j\\) is the number of minor alleles at position j\r\nA is the genetic ancestry\r\nrepeat for all positions j = 1, …, p\r\nHowever, what should we do when we do not have information on ancestry\r\nfor a particular data set? Then, we have to infer their ancestry, and\r\nhere is how powerful PCA is.\r\nAn essential approach with inferring ancestry: Principal Component Analysis\r\nWhat do we do when we do not know one’s ancestral decomposition? Well,\r\nluckily, after analyzing many individuals’ ancestral and making them\r\ninto a huge database, statisticians can utilize previous knowledge and\r\nmake guesses on an individual’s ancestry even though it is unknown. How\r\ndo we achieve this? One of the essential, and popular methods to analyze\r\nsuch problems is to use Principal Component Analysis, short for PCA.\r\nPCA is a machine learning algorithm, which can predict outcomes for new\r\ndata, using information we gained from previous data.\r\nHere are some important key highlights about PCA:\r\nIt is a technique to reduce the complexity of high dimensional data\r\nto low dimensions by “summarizing the variables”, achieved by\r\nexpressing principal components (PCs) in terms of linear\r\ncombination of predictors.\r\nThe criteria for determining the expression of PCs is to choose the\r\none which maximizes the variation of the variable values. Therefore,\r\nthis technique is called “dimension reduction” since we have fewer\r\ndimensions from variables.\r\nIt is called an unsupervised algorithm, meaning there is no\r\nresponse variable we are interested in, instead, we want PCA tells\r\nus how my distribution of data looks like by forming some clustering\r\ngroups.\r\nHere is a youtube video\r\nexplains PCA very well (adapted from Prof. Leslie Myint).\r\n\r\nFor your interest and better understanding of PCA conceptually, here\r\nis a link to a discussion\r\nforum\r\nto analog PCA concepts with characteristics in wines!\r\n\r\nWhat is beneficial and special about PCA?\r\nRecall from last section where we mention that ancestry may be a\r\nconfounder in GWAS: since PCA captures the information that summarizes\r\nancestry information,we can include them in our analysis easily just as\r\nadding a linear term to the original marginal regression model (but this\r\nis just for ONE SNP!).\r\n\\[\r\nE[Y|X_j,PC_1,PC_2]=\\beta_0+\\beta_1x_j+\\beta_2PC_1+\\beta_3 PC_2+...\r\n\\]\r\nBy PCA, we can efficiently and effectively replace the predictors we\r\nhave with new variables (PCs). In other words, we can create fewer, new\r\nvariables that help us capture all the traits from our samples. Since\r\nthe PCs are linear combination of the old predictors, the PC will look\r\nlike:\r\n\\[\r\nPC_1 = a_{11}x_1+a_{12}x_2+...+a_{1p}x_p\\\\\r\n\\]\r\n\\[\r\nPC_2 = a_{21}x_1+a_{22}x_2+...+a_{2p}x_p\\\\\r\n...\\\\\r\n\\]\r\n\\[\r\nPC_p = a_{p1}x_1+a_{p2}x_2+...+a_{pp}x_p\\\\\r\n\\]\r\nNote: \\(PC_1\\) ALWAYS explains the most variability in our data:\r\nconsidering all possible linear combinations; \\(PC_2\\) ALWAYS explains the\r\nsecond most variability in the data; same as \\(PC_3\\). The variance\r\nexplained by PC decreases with further PCs.\r\nThe amazing thing about PCA here is that the algorithm automatically\r\nclusters the data into different dimensions, allowing us to see patterns\r\nvery easily. In this context, the “patterns” mean genetic ancestry!\r\nFor an exemplar visualization, here is a figure from the paper “Genes\r\nmirror geography within Europe.” (Novembre et al.,\r\n2008.)\r\nexample of how PC1 and PC2 reflecting European genetic\r\nancestry\r\nThe cited paper mainly characterized 3000 European individuals\r\ngenotyped at lots of DNA sites to find out patterns of their\r\npopulation structure. The letters in the figure recorded their\r\nreported geographic information.\r\n\r\nPCA Vocab\r\nAfter learning what PCA is, there are some terms you should know for PCA\r\nas well!\r\nScore: The value for an observation to have for PC.\r\nLoadings: The coefficients \\(a_{11},a_{22},...a{pp}\\). They can be\r\neither positive or negative, for sure. The loadings tell you the\r\ncontribution of each original variables to the PC (take the absolute\r\nvalue of the loadings, the greater it is, the more weight this PC\r\ntakes this variable in concerns).\r\nVariance explained: The variability of the whole data a PC\r\ncaptures. This decreases with subsequent PCs.\r\nThat’s it!\r\nLab: Applying PCA\r\nYou’ve come to a lot of understanding of our work today! In this\r\nsection, we will take a look at what PCA results look like and explore\r\ntheir meanings.\r\nUnderstand PCA output\r\nHere is a screenshot from my PCA analysis demo. I have 15 SNPs (SNP\r\n1-15). Can you see how many PCs do we have? What do numbers in each\r\ncolumn mean?\r\nPCA output formSummary:\r\nThere are 15 PCs for our data set.\r\nThe “standard deviation” column explains the variability (more\r\nspecifically, standard deviation is the sqaure root of variance\r\nexplained) each PC captures. PC1 has the greatest standard as we\r\nexpect, it decreases with the sequence.\r\nThe “rotation” column are the loadings. For example, in PC1, the\r\nmost important variables are SNP 1 and SNP2 because they share the\r\nmost absolute value of the loadings (since PC1 captures the most\r\nvariability, it indicates that SNP 1 and SNP 2 contribute to it very\r\nmuch!!). This picture only captures loadings from PC1 to PC6.\r\nPC score visualization\r\nLet’s create plot showing the score for the top PCs: PC1, 2. to\r\nvisualize data patterns. What do you learn from this plot?\r\nPC1 scores V.S. PC2 scoresThe magic of PCA shows that even if we have no knowledge about the\r\nancestry, PCA automatically grouped them by the PCs. It seems like we\r\nhave a pattern between two population groups (mainly by PC1, that is\r\nwhere the score differs the most across two groups).\r\nHowever, given this strong and powerful algorithm, PCA results sometimes\r\nstill get its own limitations. Since an individual’s genotype may depend\r\non their ancestry, people with mixed ancestry inherit different genetic\r\nmaterials from their parents (or grandparents), therefore, for some SNPs\r\nthey may carry differently. If we were to carry out PCA on this group of\r\npeople, we need to more carefully intepret the results.\r\nThe topic about “alleles’ narrative potential” is also important, or,\r\nproblematic in GWAS and ancestry studies. For example, in biology, there\r\nare situations where one type of allele may functionally outdo the other\r\none, and resulting different phenotypic traits in them (complete\r\ndominance). There are also concepts in biology such as multiple\r\nalleles, incomplete dominance, codominance. Therefore, when we carry\r\nout GWAS and adjusting for ancestry, we may or may not intepret their\r\ngenotype the same way, thus posing a challenge for researchers to\r\nunderstand the results fully and correctly.\r\nOutro\r\nWe briefly explored a widely-used method for inferring ancestry: PCA.\r\nWe’ve seen how ancestry can be a potential confounder in our GWAS\r\nanalysis, and the way we should address this problematic factor is to\r\ninclude them into the model by adding PC elements. In real life study,\r\nresearchers have also been using this technique to group elements,\r\nshrinking variables, and effectively learn information from a huge data\r\nset.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-13T22:01:01-06:00"
    },
    {
      "path": "gwas.html",
      "title": "Genome-wide Association Studies",
      "description": "Let's explore one of the most common statistical tool in genetic world!",
      "author": [],
      "contents": "\r\nGenome-wide Association Studies (GWAS)\r\nWhat is GWAS?\r\nGWAS is a statistical approach to analyze the association of genetic variations and our trait of interest. There are many types of variation in genetics world, but SNP, also known as single- nucleotide polymorphism, is the most common type of genetic variation across many organisms, including us. Here is some nice introduction to SNP:\r\n\r\n“Single nucleotide polymorphisms, frequently called SNPs (pronounced”snips”), are the most common type of genetic variation among people. Each SNP represents a difference in a single DNA building block, called a nucleotide. For example, a SNP may replace the nucleotide cytosine (C) with the nucleotide thymine (T) in a certain stretch of DNA. SNPs occur normally throughout a person’s DNA. They occur almost once in every 1,000 nucleotides on average, which means there are roughly 4 to 5 million SNPs in a person’s genome. These variations occur in many individuals; to be classified as a SNP, a variant is found in at least 1 percent of the population. Scientists have found more than 600 million SNPs in populations around the world.”\r\n— https://medlineplus.gov/genetics/understanding/genomicresearch/snp/\r\n\r\nIn the context of GWAS, we are therefore investigating the relationship between the SNPs and the outcome (quantitative or qualitative traits). We can picture one SNP being a position along the genome where different people may carry different versions of nucleotide (building block of genes). Given the huge size of our genome, there are so many SNPs we can look for, and remember, each of them can be a single predictor.\r\nthe p >>>>>> n problem!\r\nNow, given that we have some basic understanding to GWAS, let’s understand it deeper from a statistician’s point of view. If we were given a set of SNP data, each SNPs should takes up a column, and each person’s data should be in each row. So what are the data? Now, recall the concept of SNP. The majority of the SNPs are biallelic, meaning there are two common, major alleles (versions of nucleotide) one random person may take with them. I will use an example to illustrate this point.\r\n\r\nAs for one SNP, there are two alleles: A and G. Each person inherits one allele from each parent. Then a person could have four possible genotypes (information about their alleles): AA, AG(mom = A, dad = G), GA(mom = G, dad = A), GG In real world, the genotype AG/GA does not matter much. Therefore we technically have three possible genotypes. We will denote a person’s information at the SNP with level factors: 0, 1, 2. This records how many minor allele copy one has. Therefore, in this example, let’s say if A is the major allele and G is the minor allele, then a person with the genotype AA will be given “0” in that SNP variable; AG will be given “1”; GG will be “2”. An example of GWAS data would look like this:\r\n\r\nA simple GWAS data frame\r\n\r\nSNP1\r\nSNP2\r\nSNP3\r\nSNP4\r\nBody Weight (kg)\r\nPerson1\r\n0\r\n0\r\n1\r\n0\r\n59.2\r\nPerson2\r\n0\r\n2\r\n1\r\n1\r\n46.9\r\nPerson3\r\n1\r\n0\r\n1\r\n0\r\n64.5\r\nPerson4\r\n2\r\n0\r\n2\r\n0\r\n72.8\r\nPerson5\r\n0\r\n1\r\n0\r\n2\r\n71.3\r\nPerson6\r\n0\r\n1\r\n0\r\n0\r\n44.5\r\nTherefore, in GWAS, it is more likely to be more predictors than observations (p>>n), creating a thin, but wide data frame. This p>n fact will lead us some problems, not only in terms of difficulty to carry out statistical regression analyses, but also about the accuracy of our tests due to multiple testing.\r\nOne of the biggest themes in GWAS is the problem about multiple testing. Multiple testing is in its core problematic since the threshold where we define p-value with our hypotheses (\\(H_0\\) and \\(H_A\\)) should be lowered. We will talk about this later in this topic.\r\nGWAS analysis methods\r\nAfter hearing about all these introductions, we now will introduce two ways (and one of them, failed, actually) to investigate GWAS. The first method is to fit linear regression, and we will understand why it does not work from matrix perspective. Then, we will explore the second method: marginal regression, which fit each linear regression model with respect to one predictor at a time.\r\nMethod 1: Matrix\r\nWhen consider linear model with such huge data, it is always easier to formulate the data into three matrices. First, the vector of outcomes \\(\\mathbf{y}\\)\r\n\\[\r\n\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix},\r\n\\]\r\nSecond, the vector of covariates \\(\\boldsymbol\\beta\\)\r\n\\[\r\n\\boldsymbol\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix},\r\n\\]\r\nLastly, the matrix of covariates (sometimes referred to as the “design matrix”) \\(\\mathbf{X}\\)\r\n\\[\r\n\\mathbf{X} = \\begin{pmatrix} 1 & x_{11} & \\cdots & x_{p1} \\\\ 1 & x_{12} & \\cdots & x_{p2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{1n} & \\cdots & x_{pn} \\end{pmatrix}.\r\n\\]\r\nIf we want to get the estimation for the best fit of linear regression model, we want to minimize the sum of squared residuals. Using matrix notation, we can formulate the least squares problem as follows:\r\n\\[\\text{argmin}_{\\boldsymbol\\beta} (\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta)^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta),\\]\r\nIf we want to know the \\(\\beta\\) which minimizes the expression above, one way is to differentiate the expression with respect to \\(\\beta\\). And we set that derivative to 0 to solve for \\(\\beta\\). Here is an example of my work.\r\nmy workNow, we get the expression of \\[\\hat{\\boldsymbol\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}.\\] But in fact, \\(\\beta\\) is undefined. Why?\r\nAnswer: Recall the dimension of matrix \\(X\\), it is \\(n*(p+1)\\), where n is the number of observation and p is the number of SNPs. The matrix \\(X\\) has more columns (p) than its row, and therefore, the columns are linearly dependent. And thus, \\(\\mathbf{X}^\\top \\mathbf{X}^{-1}\\) is not defined. Thus, we cannot find the coefficient \\(\\beta\\) for this equation.\r\nTo conclude, we failed to fit GWAS data into one linear model. However, there is a second method that works out in fact, which is Marginal Regression.\r\nMethod 2: Marginal Regression\r\nFrom above, we saw that if we were to fit a linear regression line on our y trait versus all the predictors (all the SNPs), we will fail to do so since the matrix \\(X^{T}X\\) will not be invertible in most of the cases, and therefore failed to calculate the coefficient matrix. (The reason is that we have more co-variates/ number of columns than observations/number of rows, so \\(X^{T}X\\) is not full rank, meaning the span cannot fit into the columns and this results an invertible matrix)\r\nHowever, there is an alternative way to get around this constraint. Instead of fitting all SNPs into one model, what about fitting one for each SNP? And, you are right, this method does allow us for more flexibility with GWAS, and this method is Marginal Regression.\r\nWe can define a marginal regression as follows: \\[E[y | x_j] = \\beta_0 + \\beta_1 x_j\\]\r\nwhere \\(\\beta_0\\) is the intercept, \\(x_j\\) is the number of copy of minor alleles at the position j (the SNP). Then, if we were to interpret \\(\\beta_1\\), it would mean “the change in the average of y as we increase one copy of minor allele at \\(j\\)th position”.\r\nWhy this method works then?\r\nThis has to do with linearly dependency in linear algebra. If we were to look into what matrix \\(X\\) for each of the SNPs looks like, it only has two columns now: the first column with all 1’s (serves as a placeholder for the intercept), and the second column with SNP data (0s, 1s, 2s). Therefore, unless everyone in the group has exactly same genotype (all 0s, all 1s, all 2s), where the columns of X are linearly dependent, \\(X\\) will be linearly independent and therefore we can predict the coefficients easily by regression.\r\nGiven this concept, it seems like we are happy with everything. However, there is one case when this method still does not apply for. Do you know? Hint: when is matrix X linearly dependent with each other. If we only have two columns, and one of them is just 1’s, what does it mean in context if we coud not find the coeffcient?\r\nAnswer: It is when everyone in the dataset has the same genotype (for example, everyone carries 1 copy of the minor allele, making the dataset all 1 at that position) in that SNP position. If that were the case, the columns of matrix \\(X\\) would be dependent with each other, leading \\(\\beta\\) undefined.\r\nAfter fitting a lm model to every SNP, now we are able to picture the outcome of GWAS data. The most common plot is Manhattan plot. Below is an example, where x axis denotes the position of the SNP, each dot is one SNP, and the y-axis is the -log(p-value). The greater the y-value is, the stronger the association of that SNP and the trait is:\r\nAn example of Manhattan plotManhattan Plot is a way to visualize results from GWAS. It plots the p-values (negatively, log transformed) along the SNP’s position across the chromosomes. The reasons for such transformation are : 1) The absolute values of p-values are generally small and have small difference, log transformation will greatly aid the visualization of different p-values 2) negative sign will put the more statistically significant SNPs towards the top, making them easier to be detected on the graph.\r\nTo conclude, if we were to fit a linear model to GWAS data, we should acknowledge the fact that p>>>n, in which we need to consider marginal regression and fit multiple models.\r\nBreak!\r\nHooray! It seems like we have found a decent solution to tackle GWAS studies by looking at how each co-variate (each SNP) correlates with our trait. However, marginal regression still gets its limits in real world studies. Can you think of any?\r\nWell, since in marginal setting, we only examine one SNP and generates data on its own. However in real studies, SNPs are correlated (and its paradoxical that we assume they aren’t) so examining on single SNP is less trustworthy when it has correlation with certain SNPs and are not tested together.\r\nMultiple testing\r\nHello back! After learning to apply your statistic knowledge to carry out regression studies on genetic data, you actually get the core of one of the most popular topics in biostatistics!\r\nFrom the manhattan plot you’ve seen above, we may wonder: since the y-axis is -log(p), then what threshold do we use for the testing to ensure whether that SNP has statistically significant meaning? Do we still use 0.05?\r\nThe answer is NO. Why? Now, before diving into our topic, we will recall our old friend Hypothesis testing.\r\nRecap from 155: hypothesis testing\r\nIn a single hypothesis testing, there are two hypotheses we first made:\r\n\\[H_0\\]\r\nNull hypothesis – we claim there is no association between our predictor of interest and the outcome, and \\[H_A\\]\r\nAlternative hypothesis – we claim there is a meaningful association between our predictor of interest and the outcome\r\nAfter our data yielded us results (including p-value), we need to determine the p-value of our choice of threshold \\(\\alpha\\) (which is normally, 0.05). If \\(p< \\alpha\\), then we reject \\(H_0\\); Vice versa.\r\nThere are two types of error in hypothesis testing: Type I error =\\(P(Reject H_0 | H_0 true)\\) = we reject the null when the null is true. Type II error \\(P(Keep H_0| H_A true)\\) = we fail to reject the null when the alternative is true.\r\nType I errors normally have to do with the threshold \\(\\alpha\\), we can avoid type I error by setting a smaller threshold. Type II errors are more systematic: we need to increase our statistical power (for example, increase sample size). However, we do not want either case to happen.\r\nWhy we can’t use 0.05 in GWAS?\r\nThere are very important reasons that GWAS cannot apply 0.05 as the threshold. The actual threshold is almost \\(10^{-8}\\) fold smaller.\r\n\r\nFor information:\r\nIn fact, in GWAS studies, the statisitican has figured out the threshold p-value of \\(7.2 *10^{-8}\\).(Dudbridge & Gusnanto, 2008; Pe’er et al., 2008; https://pubmed.ncbi.nlm.nih.gov/18300295/) The paper selected this threshold out of an adjustment method called Bonferroni correction.\r\n\r\nWhy not 0.05? This has to do with multiple testing. In GWAS, if we have p SNPs, then we will do p hypothesis testings. However, the problem is that the probability of getting type I error in these p tests is very high. Why? Here is a mathematical explanation:\r\nSuppose we have p tests where tests are independent, the probability of occuring a type I error for a test = our threshold. Then the probability of getting at least one type I error across p tests (third line, this is a term called Family-wise error rate (FWER)) is:\r\n\\[\r\nP(Reject H_0 | H_0 true) = 0.05\\\\\r\n\\]\r\n\\[\r\nP(no T1E) = 1-0.05=0.95\\\\\r\n\\]\r\n\\[\r\nP(have T1E) = 1-P({no}T1E_{test1})P({no}T1E_{test2})...P({no}T1E_{testp})= 1- 0.95^p\\\\\r\n\\]\r\nThis explains in multiple testing, we need a very small threshold to control type I error rate at a reasonably accepted level while holds for GWAS studies.\r\nLab: see the problem!\r\nThe following content is adapted from “Lab 3” from Prof. Kelsey Grinde.\r\nTo start, let’s generate a small dataset with 165 rows (i.e., 165 people) and 83 columns (i.e., 83 SNPs). (The reasons for these numbers will become apparent later.) For now, we’ll assume that all of these SNPs have the same minor allele frequency of 10% and we’ll generate each of these SNPs independently from the others.\r\n\r\n\r\n# function to simulate one genetic variant\r\nsim_one_variant <- function(n_ppl, MAF){\r\n  snp <- rbinom(n = n_ppl, size = 2, p = MAF)\r\n  return(snp)\r\n}\r\n\r\n# replicate 83 times to create 83 independent SNPs\r\nset.seed(494)\r\nsnps <- replicate(83, sim_one_variant(n_ppl = 165, MAF = 0.1))\r\n\r\n\r\nSuppose we want to conduct a “genome-wide” association study, using marginal regression to separately test the association between each of these 83 SNPs and a trait. Before we work on determining which significance threshold we should use in this setting, let’s explore what happens when we use a p-value threshold of 0.05.\r\nConsider the scenario where the null hypothesis is universally true: none of the 83 SNPs are associated with our trait. If we use a p-value threshold of 0.05, how often will we incorrectly state that at least one of these SNPs is associated with the trait (even though we know, in reality, that’s not true)? To investigate this question, let’s start by simulating a quantitative trait that does not depend on any of the SNPs:\r\n\r\n\r\nset.seed(1) # set random number seed for reproduciblity\r\ny <- rnorm(n = 165, mean = 0, sd = 1) # simulate Y from a standard normal distribution\r\n\r\n\r\nNext, implement GWAS to investigate the association between each SNP and the simulated null trait:\r\n\r\n\r\n# set up empty vector to store p-values\r\npvals <- c()\r\n\r\n# loop through each of the 83 SNPs\r\nfor(i in 1:83){\r\n  mod <- lm(y ~ snps[,i]) # fit model looking at SNP i\r\n  pvals[i] <- tidy(mod)$p.value[2] # record p-value\r\n}\r\n\r\n# check how many p-values are below 0.05\r\nsum(pvals < 0.05)\r\n\r\n[1] 3\r\n\r\nNote that since we simulate a fake, null trait, so we expect sum(pvals < 0.05) to be 0. That is, there should be no SNP with a p-value smaller than our significant threshold because these SNPs are designed not causal. However, the code tells us there are 3 significant SNPs.\r\nHow many wrongly discovery of finding a meaningful SNP did you see? What does the mean tell us?\r\nAnswer: Ideally, we would hope not to see any p-values below 0.05 since none of our 83 SNPs are truly associated with the trait. However, we ended up with 3 SNPs (out of 83) with a p-value smaller than 0.05. If we used a significance threshold of 0.05, this would mean that we incorrectly reject the null hypothesis for 3.61 percent of the SNPs.\r\nWhat happens when we repeat the process many times (i.e. carry out many hypothesis tests as in real GWAS studies)?\r\nLet’s repeat this process many times and see how often we end up with at least one p-value being below 0.05. Let’s wrap our code, above, into a function called do_one_sim to help us do this.\r\nAnd then, we repeat:\r\n\r\n\r\ndo_one_sim <- function(){\r\n  # simulate null trait\r\n  y <- rnorm(n = 165, mean = 0, sd = 1)\r\n  # implement GWAS\r\n  pvals <- c()\r\n  for(i in 1:83){\r\n    mod <- lm(y ~ snps[,i])\r\n    pvals[i] <- tidy(mod)$p.value[2]\r\n  }\r\n  # check if any pvals are < 0.05\r\n  any(pvals < 0.05)\r\n}\r\n# repeat simulation 500 times\r\nset.seed(494)\r\nsimres <- replicate(500, do_one_sim()) \r\nsum(simres)\r\n\r\n[1] 493\r\n\r\nWhat number do you got from sum(simres)?\r\nThis actually counts the number of simulation (out of 500) that occurs type I error. It should be 493/500 = 0.986. In other words, there is 98.6% chance that we have at least 1 type I error across 83 tests.\r\nSetting the correct threshold\r\nIn this section, we will introduce two methods to address the problem with multiple testing.\r\nMethod 1: Bonferroni Correction\r\nBonferroni correction is very easy. Continued with the asssumption above (tests are independent), The new threshold would be: \\[\r\n\\alpha_{new} = \\frac{\\alpha_{old}}{n}\r\n\\] where \\(n\\) is the number of independent tests. While it is very easily calculated, its main con is that sometimes the new threshold can be too small, thus making our decision too conservative if tests are not independent. This means we are oppositely, being too strict on the threshold and may prone to filter out those SNPs with meaningful association (i.e. prone to type II error)! This is also concerning because in GWAS genetic studies, genes are highly correlated together. This nature limits the usefulness of Bonferroni in some strict studies. However, if the tests are not correlated, this is definitely preferred!\r\nFor example: If referring to the lab above, then the Bonferroni corrected p-value threshold is\r\n\\[\r\n\\alpha =  \\frac{0.05}{83}  = 6*10^{-4}\r\n\\]\r\nMethod 2: Simulation-based approach\r\nSimulation-based approach has the advantage that avoids being too conservative in correlated tests. In other words, the threshold set using this method applies to correlated tests well enough! Let’s come and explore this.\r\nThis approach can be summarized into four steps:\r\nSimulate a null trait. This is “fake data”, because we want a pure data set where the trait is not associated with any of the SNPs.\r\nRun GWAS using marginal regression on the trait with each of the SNP.\r\nRepeat steps above for many times. For each time, record the smallest p-value.\r\nSort the p-values from the smallest to largest. Find the 5th percentile, where the p-values smaller it takes up 5% of the whole. This is the threshold!\r\nThe underlying side of this method is to create a range of p-values that are known to be null. So every value in this list actually does not indicate significance, and applying them makes a type I error. However, by selecting the 5th percentile from this list of numbers, we do ensure that we control the type I error rate to be 5% as well. In this way, we know the correct threshold.\r\nLab: correlated tests (in GWAS)\r\nIn correlated tests, how well do Bonferroni and Simulated-based approach perform? How different? Let’s explore using a real SNP dataset!\r\n\r\n\r\n#Load data and data subsetting\r\nfam <- 'file/HapMap_3_r3_1.fam'\r\nbim <- 'file/HapMap_3_r3_1.bim'\r\nbed <- 'file/HapMap_3_r3_1.bed'\r\n\r\n# then read in the files, using select.snps to select only the first 100\r\nhapmap <- read.plink(bed, bim, fam, select.snps = 1:100)\r\n\r\n\r\nNow we got our hapmap data, which records the SNPs information for different individual samples.\r\nNext, let’s test out their correlation: linkage disequilibrium, or LD. This greater the LD is, the more correlated two SNPs are. The snpStats package has a function called ld that will calculate LD for us:\r\n\r\n\r\n# calculate LD\r\nhapmap.ld <- ld(hapmap$genotypes, depth = 99, stats = \"R.squared\", symmetric = TRUE)\r\n\r\n# look at the first 5-5 data\r\nhapmap.ld[1:5, 1:5]\r\n\r\n5 x 5 sparse Matrix of class \"dsCMatrix\"\r\n           rs2185539  rs11510103 rs11240767   rs3131972   rs3131969\r\nrs2185539          .          NA         NA          NA          NA\r\nrs11510103        NA .                   NA 0.005662224 0.007526042\r\nrs11240767        NA          NA          .          NA          NA\r\nrs3131972         NA 0.005662224         NA .           0.800991691\r\nrs3131969         NA 0.007526042         NA 0.800991691 .          \r\n\r\n# plot LD (fun color palette)\r\ncolor.pal <- natparks.pals(\"Acadia\", 10)\r\nimage(hapmap.ld, lwd = 0, cuts = 9, col.regions = color.pal, colorkey = TRUE)\r\n\r\n\r\n\r\nFrom the yield, we can see there are some SNP pairs with degree of LD.\r\nAnd now, let’s do the simulation based approach!\r\n\r\n\r\n# data subsetting\r\nhapmap.geno <- as(hapmap$genotypes, \"numeric\")\r\nmaf <- col.summary(hapmap$genotypes)$MAF\r\nmonomorphic <- which(maf == 0) \r\n\r\n# filter genotype matrix to remove monomorphic SNPs\r\nhapmap.geno <- hapmap.geno[,-monomorphic]\r\n\r\n# write a function to do one simulation replicate\r\ndo_one_rep_hapmap <- function(){\r\n  # simulate null trait\r\n  y <- rnorm(n = 165, mean = 0, sd = 1) \r\n  # implement GWAS\r\n  pvals <- c()\r\n  for(i in 1:83){\r\n    mod <- lm(y ~ hapmap.geno[,i])\r\n    pvals[i] <- tidy(mod)$p.value[2]\r\n  }\r\n  # record the smallest p-value\r\n  min(pvals)\r\n}\r\n\r\n# repeat many times\r\nset.seed(494) \r\nhapmap.reps <- replicate(500, do_one_rep_hapmap())\r\n\r\n\r\n\r\n\r\n# then use the quantile function to find the lower 5th percentile\r\nquantile(hapmap.reps, 0.05)\r\n\r\n         5% \r\n0.001048875 \r\n\r\nNow, let’s compare your result from this approach with Bonferroni approach. Any difference in the results? Which one is smaller (stricter)?\r\nI got \\(1.05*10^{-3}\\) for this approach, and Bonferroni yields us \\(6*10^{-4}\\). The Bonferroni one is more conservative in this correlated tests.\r\nThe reason for this is that with correlated testing, even though we fit 83 SNPs, in reality, the effective tests we did were less than 83. Thus the threshold from simulated based approach is less conservative. In constrast, Bonferroni just corrected for 83 tests by diving the threshold by 83, yielding a smaller threshold.\r\nTo conclude, with correlated data, simulated based approach is better; On the other hand, if the data are not correlated, Bonferroni is a better approach given its computational efficiency (Simulation takes much more time!).\r\nOne of the limitation here is that no matter which approach we used, determining an association by p-value should really expand more flexible room. Although most of the time the threshold serves as a fixed, important watershed between meaningful statistical associations and null results, as for researcher, especially in GWAS where association can imply things very differently from the real biochemical mechanism behind, we need to keep in mind to look at the values of -log(P) and further investigate those SNP that lie under the threshold for future reference. While GWAS presents not only the problem of p >>> n, one potential direction further is to include more samples to increase our statistical power.\r\nOutro\r\nCongratulations on finishing off the first main chapter in statistical genetics: GWAS. While we went through some important aspects and limits of GWAS, it is worth noting that GWAS still applies to other topics in biostatistics, for example, the search for gene-gene and gene-environment interactions.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-13T22:01:05-06:00"
    },
    {
      "path": "index.html",
      "title": "Kristy Ma's Statistical Genetics Notes",
      "description": "Welcome to the website. This is a portfolio consists of notes in STAT494 (statistical genetics).\n",
      "author": [],
      "contents": "\r\nIntroduction to statistical genetics\r\nStatistical Genetics is a subject and field of study investigating how variation in genome associates people’s research of interests by statistical methods.\r\nHere is an excerpt from the book <An Introduction to Statistical Genetic Data Analysis> (Mills, Barban, et.al (2020)) describing genetics data analysis background for us:\r\n\r\n“Human genetic research is now relevant beyond biology, epidemiology, and the medical sciences, with applications in such fields as psychology, psychiatry, statistics, demography, sociology, and economics. With advances in computing power, the availability of data, and new techniques, it is now possible to integrate large-scale molecular genetic information into research across a broad range of topics.”\r\n\r\nThrough this portfolio, we will explore a tiny subfield of statistical genetics:Genome wide association studies (GWAS). It is one of the most popular statistical tools. The method investigates a type of genetic variants, called SNP (single-nucleotide polymorphism), and its associations with the trait of interest. The trait can be either binary (for example, if a person is affected by a disease), or quantitative (for example, body weight/ height).\r\nIn chapter 1, we will learn the context of Genome-wide association studies, and its basics. In chapter 2, we will explore the effect of population, ancestry and how that related to GWAS application and analysis.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-12-13T22:01:07-06:00"
    }
  ],
  "collections": []
}
